{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üéµ LoFi Music Generator - GPU Training on Colab\n",
    "\n",
    "**Train your LoFi music AI model with FREE GPU!**\n",
    "\n",
    "This notebook will:\n",
    "- ‚úÖ Use Google's FREE GPU (100x faster than CPU)\n",
    "- ‚úÖ Train on 178k MIDI files from Lakh dataset\n",
    "- ‚úÖ Save trained model for download\n",
    "- ‚úÖ Complete in 8-12 hours (not 43 days!)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° IMPORTANT: Enable GPU First!\n",
    "\n",
    "1. Click **Runtime** ‚Üí **Change runtime type**\n",
    "2. Select **T4 GPU** or **GPU** from Hardware accelerator\n",
    "3. Click **Save**\n",
    "\n",
    "**Then run the cells below in order!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU is available\n",
    "import torch\n",
    "print(f\"üîç GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå NO GPU! Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üíæ Mount Google Drive (IMPORTANT!)\n\n**This is CRITICAL for training 178k files!**\n\nWhy you need this:\n- Training takes 8-12 hours on 178k MIDI files\n- Colab sessions disconnect after 12 hours (or randomly)\n- Without Drive, all progress is LOST on disconnect\n- With Drive, you can resume exactly where you left off\n\nWhat gets saved to Drive:\n- ‚úÖ Tokenized MIDI files (so you don't re-tokenize 178k files!)\n- ‚úÖ Model checkpoints every 500 steps\n- ‚úÖ Training logs and metrics\n- ‚úÖ Final trained model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive\nfrom google.colab import drive\nimport os\n\nprint(\"üîó Mounting Google Drive...\")\nprint(\"You'll need to click the link and authorize access.\\n\")\n\ndrive.mount('/content/drive')\n\n# Create directory structure in Drive\ndrive_dir = '/content/drive/MyDrive/LoFi_Training'\nos.makedirs(drive_dir, exist_ok=True)\nos.makedirs(f'{drive_dir}/checkpoints', exist_ok=True)\nos.makedirs(f'{drive_dir}/tokenized_data', exist_ok=True)\n\nprint(f\"\\n‚úÖ Google Drive mounted!\")\nprint(f\"üìÅ Training data will be saved to: {drive_dir}\")\nprint(\"\\nIf Colab disconnects, just re-run the cells and training will resume automatically!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üì• Clone Repository\n\n**IMPORTANT:** If you get authentication errors when cloning, choose ONE solution:\n\n### Option 1: Make Repository Public (Easiest)\n1. Go to https://github.com/andy-regulore/lofi\n2. Click **Settings** ‚Üí **General**\n3. Scroll to **Danger Zone** ‚Üí **Change repository visibility**\n4. Click **Change visibility** ‚Üí **Make public**\n\n### Option 2: Use Personal Access Token\n1. Go to https://github.com/settings/tokens\n2. Generate new token (classic) with **repo** scope\n3. Copy the token\n4. In the cell below, replace the clone command with:\n   ```\n   !git clone https://YOUR_TOKEN_HERE@github.com/andy-regulore/lofi.git\n   ```\n\nThen run the cell below ‚¨áÔ∏è",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": "# Clone repository (handles both first run and re-runs after disconnect)\nimport os\n\nif os.path.exists('/content/lofi'):\n    print(\"üìÅ Repository already exists (from previous run)\")\n    print(\"Skipping clone, changing to directory...\\n\")\n    %cd /content/lofi\n    \n    # Pull latest changes just in case\n    print(\"üîÑ Pulling latest changes...\")\n    !git pull origin main\nelse:\n    print(\"üì• Cloning repository for first time...\\n\")\n    # NOTE: If you get authentication errors, you need to either:\n    # 1. Make your repository PUBLIC on GitHub (Settings ‚Üí General ‚Üí Danger Zone ‚Üí Change visibility)\n    # 2. OR use a personal access token: !git clone https://YOUR_TOKEN@github.com/andy-regulore/lofi.git\n    \n    !git clone https://github.com/andy-regulore/lofi.git\n    %cd /content/lofi\n    \n    # Use main branch (all fixes are here)\n    !git checkout main\n\n# Verify we're in the right place\nprint(\"\\n‚úÖ Current directory:\")\n!pwd\nprint(\"\\n‚úÖ Checking for config.yaml:\")\n!ls -la config.yaml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies (this takes 3-5 minutes)...\")\n",
    "!pip install -q torch transformers datasets accelerate\n",
    "!pip install -q miditok miditoolkit pretty_midi\n",
    "!pip install -q librosa soundfile scipy numpy pandas\n",
    "!pip install -q pyyaml scikit-learn tqdm tensorboard\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## üìÇ Step 2: Get Training Data\n",
    "\n",
    "**Choose ONE option:**\n",
    "- **Option A:** Download Lakh MIDI Dataset (176k files, ~20GB)\n",
    "- **Option B:** Upload your own MIDI files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-lakh"
   },
   "outputs": [],
   "source": [
    "# OPTION A: Download Lakh MIDI Dataset (recommended)\n",
    "print(\"üì• Downloading Lakh MIDI Dataset (~20GB, takes 10-20 minutes)...\")\n",
    "!mkdir -p data/training\n",
    "!wget -q --show-progress http://hog.ee.columbia.edu/craffel/lmd/lmd_full.tar.gz\n",
    "print(\"\\nüì¶ Extracting dataset...\")\n",
    "!tar -xzf lmd_full.tar.gz -C data/training/\n",
    "!rm lmd_full.tar.gz\n",
    "print(\"‚úÖ Dataset ready!\")\n",
    "\n",
    "# Count files\n",
    "import os\n",
    "midi_count = sum(1 for root, dirs, files in os.walk('data/training') \n",
    "                 for f in files if f.endswith(('.mid', '.midi')))\n",
    "print(f\"\\nüéµ Found {midi_count:,} MIDI files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# OPTION B: Use Google Drive (skip if you used Option A)\n",
    "# Uncomment if you have MIDI files in Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Copy from your Google Drive to Colab\n",
    "# !mkdir -p data/training\n",
    "# !cp -r /content/drive/MyDrive/your-midi-folder/* data/training/\n",
    "\n",
    "# # Count files\n",
    "# import os\n",
    "# midi_count = sum(1 for root, dirs, files in os.walk('data/training') \n",
    "#                  for f in files if f.endswith(('.mid', '.midi')))\n",
    "# print(f\"üéµ Found {midi_count:,} MIDI files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## üöÄ Step 3: Train the Model!\n",
    "\n",
    "This will:\n",
    "1. Tokenize all MIDI files (1-2 hours)\n",
    "2. Train GPT-2 model (6-10 hours)\n",
    "3. Save trained model\n",
    "\n",
    "**Total time: 8-12 hours with GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-training"
   },
   "outputs": [],
   "source": "# Run training directly in Python with Google Drive checkpoint saving\nprint(\"üöÄ Starting training with checkpoint saving...\\n\")\nprint(\"üíæ All progress will be saved to Google Drive!\")\nprint(\"If Colab disconnects, re-run this cell to resume from last checkpoint.\\n\")\n\n# ‚ö° PERFORMANCE SETTING: Limit number of MIDI files to process\n# Set to None to use ALL files (178k = 12+ hours tokenization)\n# Recommended values:\n#   - 5,000 files = ~20 minutes tokenization, good for testing\n#   - 10,000 files = ~40 minutes tokenization, decent training\n#   - 20,000 files = ~80 minutes tokenization, better quality\n#   - 50,000 files = ~3 hours tokenization, high quality\nMAX_MIDI_FILES = 10000  # Change this number or set to None for all files\n\n# Make sure we're in the right directory\nimport os\nimport pickle\nimport random\nfrom pathlib import Path\n\nif not Path('config.yaml').exists():\n    print(\"‚ùå Error: Not in lofi directory!\")\n    print(\"Please run the repository clone cell first.\")\n    raise FileNotFoundError(\"config.yaml not found - wrong directory\")\n\nprint(f\"‚úÖ Working directory: {os.getcwd()}\\n\")\n\nimport yaml\nimport torch\nfrom src.tokenizer import LoFiTokenizer\nfrom src.model import ConditionedLoFiModel\nfrom src.trainer import LoFiTrainer\nfrom sklearn.model_selection import train_test_split\n\n# Google Drive paths\nDRIVE_DIR = '/content/drive/MyDrive/LoFi_Training'\nTOKENIZED_DATA_PATH = f'{DRIVE_DIR}/tokenized_data/sequences.pkl'\nCHECKPOINT_DIR = f'{DRIVE_DIR}/checkpoints'\nFINAL_MODEL_DIR = f'{DRIVE_DIR}/final_model'\n\n# Load config\nwith open('config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# Override settings for Colab with Google Drive checkpointing\nconfig['training']['output_dir'] = CHECKPOINT_DIR\nconfig['training']['device'] = 'cuda'\nconfig['training']['fp16'] = True\nconfig['training']['num_epochs'] = 15  # User set to 15\nconfig['training']['batch_size'] = 4   # User set to 4\nconfig['training']['save_steps'] = 500  # Save checkpoint every 500 steps\nconfig['training']['save_total_limit'] = 3  # Keep only last 3 checkpoints to save space\nconfig['data']['quality_filters']['require_drums'] = False\nconfig['data']['quality_filters']['min_tempo'] = 1\nconfig['data']['quality_filters']['max_tempo'] = 999\n\nprint(\"=\"*60)\nprint(\"PHASE 1: TOKENIZING MIDI FILES\")\nprint(\"=\"*60)\n\n# Check if we already have tokenized data in Google Drive\nif os.path.exists(TOKENIZED_DATA_PATH):\n    print(f\"\\nüéâ Found existing tokenized data in Google Drive!\")\n    print(f\"Loading from: {TOKENIZED_DATA_PATH}\")\n    print(\"This saves hours of tokenization time!\\n\")\n    \n    with open(TOKENIZED_DATA_PATH, 'rb') as f:\n        saved_data = pickle.load(f)\n        token_sequences = saved_data['token_sequences']\n        vocab_size = saved_data['vocab_size']\n    \n    print(f\"‚úÖ Loaded {len(token_sequences):,} token sequences\")\n    print(f\"Vocabulary size: {vocab_size}\")\n    \nelse:\n    print(\"\\nüíæ No existing tokenized data found. Starting tokenization...\")\n    print(\"(This will be saved to Google Drive for future runs)\\n\")\n    \n    # Initialize tokenizer\n    tokenizer = LoFiTokenizer(config)\n    vocab_size = tokenizer.tokenizer.vocab_size\n    print(f\"Vocabulary size: {vocab_size}\")\n\n    # Find all MIDI files\n    training_dir = Path('data/training')\n    all_midi_files = list(training_dir.glob('**/*.mid')) + list(training_dir.glob('**/*.midi'))\n    \n    if len(all_midi_files) == 0:\n        print(\"\\n\" + \"=\"*60)\n        print(\"‚ùå ERROR: NO MIDI FILES FOUND!\")\n        print(\"=\"*60)\n        print(\"\\nYou need to download training data first!\")\n        print(\"\\nüìã STEPS TO FIX:\")\n        print(\"1. Scroll up to 'Step 2: Get Training Data'\")\n        print(\"2. Run either:\")\n        print(\"   - OPTION A: Download Lakh MIDI Dataset cell (recommended)\")\n        print(\"   - OPTION B: Upload your own MIDI files from Google Drive\")\n        print(\"3. Wait for download/upload to complete\")\n        print(\"4. Then come back and re-run this training cell\")\n        print(\"\\n\" + \"=\"*60)\n        raise ValueError(\"No MIDI files found! Please download training data first (see Step 2 above).\")\n    \n    # Limit number of files if MAX_MIDI_FILES is set\n    if MAX_MIDI_FILES and MAX_MIDI_FILES < len(all_midi_files):\n        print(f\"‚ö° PERFORMANCE MODE: Using {MAX_MIDI_FILES:,} out of {len(all_midi_files):,} files\")\n        print(f\"(To use all files, set MAX_MIDI_FILES = None at the top of this cell)\\n\")\n        random.seed(42)  # Reproducible sample\n        midi_files = random.sample(all_midi_files, MAX_MIDI_FILES)\n    else:\n        print(f\"Found {len(all_midi_files):,} MIDI files (using all)\\n\")\n        midi_files = all_midi_files\n\n    # Tokenize files\n    token_sequences = []\n    success_count = 0\n    fail_count = 0\n\n    print(f\"Tokenizing {len(midi_files):,} files...\")\n    print(\"Progress updates every 100 files:\\n\")\n\n    for i, midi_file in enumerate(midi_files):\n        try:\n            # More frequent progress updates (every 100 instead of 1000)\n            if (i + 1) % 100 == 0:\n                elapsed_pct = (i + 1) / len(midi_files) * 100\n                print(f\"  [{elapsed_pct:5.1f}%] Processed {i+1:,}/{len(midi_files):,} files - \"\n                      f\"Success: {success_count:,}, Failed: {fail_count:,}\")\n            \n            result = tokenizer.tokenize_midi(str(midi_file), check_quality=False)\n            if result and 'tokens' in result and len(result['tokens']) > 0:\n                chunks = tokenizer.chunk_sequence(result['tokens'])\n                if chunks:\n                    token_sequences.extend(chunks)\n                    success_count += 1\n                else:\n                    fail_count += 1\n            else:\n                fail_count += 1\n        except Exception as e:\n            fail_count += 1\n            if fail_count <= 10:\n                print(f\"  Error with {midi_file.name}: {str(e)[:100]}\")\n            continue\n\n    print(f\"\\n‚úÖ Tokenization complete!\")\n    print(f\"  Success: {success_count:,} files\")\n    print(f\"  Failed: {fail_count:,} files\")\n    print(f\"  Generated: {len(token_sequences):,} token sequences\")\n\n    if len(token_sequences) == 0:\n        raise ValueError(\"No valid sequences generated. Check MIDI files.\")\n    \n    # Save tokenized data to Google Drive\n    print(f\"\\nüíæ Saving tokenized data to Google Drive...\")\n    print(f\"   Path: {TOKENIZED_DATA_PATH}\")\n    \n    with open(TOKENIZED_DATA_PATH, 'wb') as f:\n        pickle.dump({\n            'token_sequences': token_sequences,\n            'vocab_size': vocab_size\n        }, f)\n    \n    print(\"‚úÖ Tokenized data saved! Future runs will skip tokenization.\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 2: SPLITTING DATASET\")\nprint(\"=\"*60)\n\n# Split into train/eval\ntrain_sequences, eval_sequences = train_test_split(\n    token_sequences, test_size=0.1, random_state=42\n)\n\nprint(f\"Training sequences: {len(train_sequences):,}\")\nprint(f\"Evaluation sequences: {len(eval_sequences):,}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 3: INITIALIZING MODEL\")\nprint(\"=\"*60)\n\n# Initialize model\nmodel = ConditionedLoFiModel(config, vocab_size)\nmodel_info = model.get_model_info()\n\nprint(f\"Model: GPT-2\")\nprint(f\"Parameters: {model_info['total_parameters']:,} ({model_info['total_parameters']/1e6:.1f}M)\")\nprint(f\"Layers: {model_info['num_layers']}\")\nprint(f\"Context length: {model_info['context_length']}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 4: TRAINING MODEL\")\nprint(\"=\"*60)\nprint(f\"\\nüíæ Checkpoints will be saved to Google Drive every 500 steps\")\nprint(f\"üìÅ Location: {CHECKPOINT_DIR}\")\nprint(f\"\\nEpochs: {config['training']['num_epochs']}, Batch size: {config['training']['batch_size']}\")\nprint(\"\\n‚ö†Ô∏è If Colab disconnects, just re-run this cell - it will auto-resume from the last checkpoint!\\n\")\n\n# Check for existing checkpoints (just for user info - trainer handles this automatically)\nfrom transformers.trainer_utils import get_last_checkpoint\n\nexisting_checkpoint = None\nif os.path.exists(CHECKPOINT_DIR):\n    existing_checkpoint = get_last_checkpoint(CHECKPOINT_DIR)\n    if existing_checkpoint:\n        print(f\"üîÑ Found existing checkpoint: {os.path.basename(existing_checkpoint)}\")\n        print(f\"   Training will resume from this checkpoint!\\n\")\n\n# Initialize trainer (it will auto-detect and resume from checkpoints)\ntrainer = LoFiTrainer(model, config, vocab_size)\n\n# Train! (trainer automatically resumes from last checkpoint if it exists)\nresults = trainer.train(train_sequences, eval_sequences)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ TRAINING COMPLETE!\")\nprint(\"=\"*60)\n\nprint(f\"\\nFinal metrics:\")\nprint(f\"  Train loss: {results['train_metrics'].get('train_loss', 'N/A')}\")\nprint(f\"  Eval loss: {results['eval_metrics'].get('eval_loss', 'N/A')}\")\n\n# Copy final model to separate directory in Google Drive\nprint(f\"\\nüíæ Copying final model to: {FINAL_MODEL_DIR}\")\nimport shutil\nif os.path.exists(FINAL_MODEL_DIR):\n    shutil.rmtree(FINAL_MODEL_DIR)\nshutil.copytree(CHECKPOINT_DIR, FINAL_MODEL_DIR)\n\nprint(f\"\\n‚úÖ Model saved to Google Drive!\")\nprint(f\"üìÅ Location: {FINAL_MODEL_DIR}\")\nprint(\"\\nüëâ Run the download cell below to get your trained model!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## üìä Step 4: Monitor Training (Optional)\n",
    "\n",
    "Run this in a separate cell to check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": "# Load TensorBoard to monitor training from Google Drive\n%load_ext tensorboard\n%tensorboard --logdir /content/drive/MyDrive/LoFi_Training/checkpoints\n\n# You can also view logs locally if training hasn't started yet:\n# %tensorboard --logdir models/colab-trained/logs"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üíæ Step 5: Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip-model"
   },
   "outputs": [],
   "source": "# Download trained model from Google Drive\nimport os\nfrom google.colab import files\n\nFINAL_MODEL_DIR = '/content/drive/MyDrive/LoFi_Training/final_model'\n\nif os.path.exists(FINAL_MODEL_DIR):\n    print(\"üì¶ Zipping your trained model from Google Drive...\")\n    print(f\"üìÅ Source: {FINAL_MODEL_DIR}\\n\")\n    \n    # Zip the model\n    !cd /content/drive/MyDrive/LoFi_Training && zip -r trained_lofi_model.zip final_model/\n    \n    print(\"\\nüíæ Downloading to your computer...\")\n    files.download('/content/drive/MyDrive/LoFi_Training/trained_lofi_model.zip')\n    \n    print(\"\\n‚úÖ Model downloaded!\")\n    print(\"\\nNext steps:\")\n    print(\"1. Unzip trained_lofi_model.zip\")\n    print(\"2. Copy the 'final_model' folder to your local lofi/models/ directory\")\n    print(\"3. Rename it to 'lofi-gpt2' (or update config.yaml)\")\n    print(\"4. Start generating music with your web UI!\")\nelse:\n    print(\"‚ùå No trained model found in Google Drive!\")\n    print(f\"Expected location: {FINAL_MODEL_DIR}\")\n    print(\"\\nPlease run the training cell first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test"
   },
   "source": [
    "## üéµ Step 6: Test Generation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate"
   },
   "outputs": [],
   "source": "# Generate a test track (if scripts exist)\n# Note: You can also generate using the web UI after downloading the model\n\ntry:\n    !python scripts/04_generate.py \\\n        --config config.yaml \\\n        --model-path models/colab-trained \\\n        --output-dir output/test \\\n        --num-tracks 1 \\\n        --mood chill \\\n        --tempo 75\n\n    print(\"\\n‚úÖ Generated test track in output/test/\")\n\n    # Download the generated MIDI file\n    from google.colab import files\n    import os\n    if os.path.exists('output/test/midi'):\n        midi_files = [f for f in os.listdir('output/test/midi') if f.endswith('.mid')]\n        if midi_files:\n            files.download(f'output/test/midi/{midi_files[0]}')\n    else:\n        print(\"MIDI output directory not found\")\nexcept Exception as e:\n    print(f\"Generation failed: {e}\")\n    print(\"You can generate music using the web UI after downloading the model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next"
   },
   "source": "## ‚úÖ Next Steps\n\nAfter training completes:\n\n1. **Download the model** (Step 5 above)\n2. **Unzip** on your local machine\n3. **Place in** `lofi/models/lofi-gpt2/`\n4. **Generate music** using your local web UI!\n\n---\n\n### üíæ Google Drive Checkpoint System\n\n**How it works:**\n- ‚úÖ Tokenized MIDI data saved to Drive (1-2 hour savings!)\n- ‚úÖ Model checkpoints saved every 500 steps\n- ‚úÖ Keeps last 3 checkpoints (saves Drive space)\n- ‚úÖ Auto-resumes from latest checkpoint\n\n**If Colab disconnects:**\n1. Wait for email notification (or check manually)\n2. Re-open this notebook\n3. Re-run the cells in order\n4. Training resumes exactly where it left off!\n\n**Your Google Drive will have:**\n```\nMyDrive/\n  LoFi_Training/\n    ‚îú‚îÄ‚îÄ tokenized_data/\n    ‚îÇ   ‚îî‚îÄ‚îÄ sequences.pkl (saved for future runs)\n    ‚îú‚îÄ‚îÄ checkpoints/\n    ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-500/\n    ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-1000/\n    ‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-1500/\n    ‚îî‚îÄ‚îÄ final_model/ (ready to download)\n```\n\n---\n\n### üéØ Pro Tips:\n\n- **First run takes longest** - tokenization (1-2 hours) + training (6-10 hours)\n- **Subsequent runs are faster** - tokenized data is cached in Drive\n- **Training interrupted?** - Just re-run, it auto-resumes\n- **Want faster training?** - Reduce `num_epochs` from 10 to 5 in training cell\n- **Check progress anytime** - Use TensorBoard cell or check Google Drive folder\n- **Drive space low?** - Training uses ~5-10GB total\n\n---\n\n### ‚ö° Troubleshooting:\n\n**\"Runtime disconnected\"**\n- Normal! Colab has 12-hour limit. Just re-run cells to resume.\n\n**\"Out of memory\"**\n- Reduce `batch_size` from 8 to 4 in training cell\n- Make sure you selected GPU (not CPU) in runtime settings\n\n**\"No MIDI files found\"**\n- Make sure Step 2 (download dataset) completed successfully\n- Check `data/training/` directory has files\n\n**\"Checkpoint not found\"**\n- First run won't have checkpoints - that's normal\n- Checkpoints appear after 500 training steps"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LoFi_Training_Colab.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}